\documentclass{article}
\renewcommand{\subparagraph}{\paragraph}
\usepackage{include/nips13submit_e}
%\usepackage{theapa}
\usepackage{times}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{pstricks}
\usepackage{pst-tree}
%\usepackage{color}
%\usepackage{makeidx}  % allows for indexgeneration
\usepackage{bm}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{array}
\usepackage{colortbl}
\usepackage{framed}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
%\usepackage{sgame}
\usepackage{dsfont}
\def\sgtextcolor{black}
\def\sglinecolor{black}
%\renewcommand{\gamestretch}{2}
\usepackage{multicol}
\usepackage{lscape}
\usepackage{relsize}
\usepackage{rotating}
\usepackage{tikz}
\usetikzlibrary{calc}

% ============== Mike's commands ==============
\usepackage{nicefrac}
\newcommand{\vect}[1]{\underline{\smash{#1}}}
\renewcommand{\v}[1]{\vect{#1}}
\newcommand{\reals}{\mathds{R}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sD}{\mathcal{D}}
\newcommand{\br}{^{\text{\textnormal{ r}}}}
\newcommand{\cat}{^{\text{\textnormal{c}}}}
% ============== ==============

\newcommand{\cut}[1]{}
\newcommand{\hide}[1]{}
\renewcommand{\blue}[1]{{\textcolor{blue}{#1}}}
%\renewcommand{\blue}[1]{#1}

%% Zapf Chancery font: has lowercase script letters
\DeclareFontFamily{OT1}{pzc}{}
\DeclareFontShape{OT1}{pzc}{m}{it}{<-> s * [1.200] pzcmi7t}{}
\DeclareMathAlphabet{\mathscr}{OT1}{pzc}{m}{it}

\newcommand\transpose{{\textrm{\tiny{\sf{T}}}}}
\newcommand{\note}[1]{}
\newcommand{\hlinespace}{~\vspace*{-0.15cm}~\\\hline\\\vspace*{0.15cm}}
%\newcommand{\hlinespace}{~\vspace*{0.45cm}\\\hline\\~\vspace*{-0.9cm}}
%\newcommand{\hlinespace}{~\vspace*{0.05cm}\\\hline~\vspace*{0.5cm}}

% comment the next line to turn off notes
\renewcommand{\note}[1]{~\\\frame{\begin{minipage}[c]{\textwidth}\vspace{2pt}\center{#1}\vspace{2pt}\end{minipage}}\vspace{3pt}\\}
\newcommand{\lnote}[1]{\note{#1}}
\newcommand{\emcite}[1]{\citet{#1}}
\newcommand{\yrcite}[1]{\citeyear{#1}}
\newcommand{\aunpcite}[1]{\citeR{#1}}

\newcommand{\heavyrule}{\specialrule{\heavyrulewidth}{.4em}{.4em}}
\newcommand{\lightrule}{\specialrule{.03em}{.4em}{.4em}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% keep figures from going onto a page by themselves
\renewcommand{\topfraction}{0.9}
\renewcommand{\textfraction}{0.07}
\renewcommand{\floatpagefraction}{0.9}
\renewcommand{\dbltopfraction}{0.9}      % for double-column styles
\renewcommand{\dblfloatpagefraction}{0.7}   % for double-column styles



\usepackage{amsmath, amsthm, amssymb}
\newtheorem{thm}{Theorem}%[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{obs}[thm]{Observation}

%\theoremstyle{definition}
\newtheorem{define}[thm]{Definition}
\hyphenation{ge-ne-ral-ize}




\newcommand{\Var}{\ensuremath\text{Var}}
\newcommand{\indicator}{\ensuremath\mathds{I}}

\newcommand{\fhspace}{\vspace*{0.2cm}}
\newcommand{\newsec}{\hspace{0cm}}

% replaces tabular; takes same arguments. use \midrule for a rule, no vertical rules, and eg \cmidrule(l){2-3} as needed with \multicolumn
\newenvironment{ktabular}[1]{\sffamily\small\begin{center}\begin{tabular}[c]{#1}\toprule}{\bottomrule \end{tabular}\end{center}\normalsize\rmfamily\vspace{-5pt}}
\newcommand{\tbold}[1]{\textbf{#1}}
\newcommand{\interrowspace}{.6em}

\nipsfinalcopy

\begin{document}

\title{Raiders of the Lost Architecture:\\A Kernel for Conditional Parameter Spaces}

\author{
Kevin Swersky \\
University of Toronto \\
\texttt{kswesrky@cs.utoronto.edu} \\
\And
David Duvenaud \\
University of Cambridge \\
\texttt{dkd23@cam.ac.uk} \\
\And
Jasper Snoek\\
Harvard University \\
\texttt{jsnoek@seas.harvard.edu} \\
\AND
Frank Hutter  \\
Freiburg University \\
{\tt fh@informatik.uni-freiburg.de} \\
\And
Michael A. Osborne \\
University of Oxford \\
{\tt mosb@robots.ox.ac.uk} \\
}




\maketitle
\begin{abstract}
When performing model-based optimization, we must often search over structures with differing numbers of parameters.  For instance, we may wish to search over neural network architectures with an unkown number of layers.  To combine information between different architectures, we define a family of kernels for conditional parameter spaces.
\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Recently, Bayesian optimization has been used to learn parameters for neural networks \cite{snoek-etal-2012b}.
Because different neural net architectures have different numbers of parameters (or their parameters have different meanings), it is not clear how to combine information or extrapolate between the performance of different architectures.
Historically, practitioners have simply built a separate model for each type of architecture \cite{bergstra2011algorithms}.  However, if there is any relation between networks with different architectures, seperately modeling each architecture is wasteful.

Bayesian optimization methods rely on constructing a model of the function being optimized.  We model this function using Gaussian process priors \cite{rasmussen38gaussian}, where model assumptions are defined through the kernel.  In this paper, we introduce a simple method for building a Gaussian process model over parameter spaces with varying numbers of parameters.  We do this by defining an embedding of datapoints depending on which parameters are active, then perform standard regression in this new space.





\section{A Kernel for Conditional Parameter Spaces}


\begin{figure}
\input{figures/semicylinder}
\caption{A demonstration of the embedding giving rise to the pseduo-metric in 2 dimensions.  All points for which $\delta_i(x) =$ false are mapped onto a line varying only along $x_1$.  Points for which $\delta_i(x) =$ true are mapped to the surface of a semicylinder, depending on both $x_1$ and $x_2$.  This embedding gives a constant distance between pairs of points which have differing values of $\delta$ but the same values of $x_1$.  The parameter $\rho$ determines how much distance there is along the arc.}
\end{figure}



% Insert description of kernel here





\section{Experiments}

\paragraph{Regression}   Bayesian optimization requires building a model of the function being optimized, and better models can be expected to lead to better outcomes.  However, because of the many interating components of BO, optimizer performance might not correspond directly to the quality of the model.  In this section, we isolate to what extent the conditional kernel is a better model than the alternatives.

\begin{table}[h!]
\caption{{\small
Normalized Mean Squared Error on Neural Network data
}}
\label{tbl:nn_nmse}
\input{tables/gpml-table.tex}
\end{table}

Separate Linear and Seperate {\sc gp} split the data into 6 different datasets, one for each level, and build a separate model for each level.  The gp-hierarchical model gets all the data together because it can handle it.  

However, The gp-hierarchical model changes two things at once compared to the separate-gp-ard model: besides embedding the missing data in a different spot, it also embeds the fully-observed data on semi-circles, and has a different parameterization.  
So, it could be the case that even when the data are fully observed, embedding the data on a semi-circle and using a different parameterization might cause better or worse performance than a standard squared-exp.  To find out if this is the case, we compare separate-gp-ard and separate-hierarchical to find out if these two models have different performance even in the standard fully-observed case.




\section{Conclusion}



\bibliography{hierarchicalkernel}
\bibliographystyle{unsrt}


\end{document}

